# -*- coding: utf-8 -*-
"""TL_preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15crxFZpBH9QOY6a9X5C25-DzxABN2PP-

**gerekli kutuphanelerin eklenmesi**

---
"""

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import random
import os

from keras.preprocessing.image import ImageDataGenerator, load_img
from sklearn.model_selection import train_test_split

import pickle
import cv2 as cv
import tensorflow as tf
from google.colab.patches import cv2_imshow

"""verilerin drivedan cekilmesi"""

from google.colab import drive
drive.mount('/content/drive')

# once tek bir resim uzerinde denemeler yapalim
path_to_image = "/content/drive/MyDrive/transfer_learning/catsanddogs/PetImages/Dog/16.jpg"

image = cv.imread(path_to_image, cv.IMREAD_COLOR)
cv2_imshow(image)

type(image)

image

image.shape

"""**Resizing and Normalization**

---


"""

resized = cv.resize(image, (128,128))

type(resized)

resized

cv2_imshow(resized)

resized.shape

"""**On Isleme**

---


"""

path_to_dataset = "/content/drive/MyDrive/transfer_learning/catsanddogs/PetImages"

# dataseti bu sekilde 2 kategoriye ayiralim: kediler ve kopekler
categories = ["Cat", "Dog"]

# verileri normalize edecegimiz fonksiyonu yazalim.
def normalize(x):
  x = (x-x.min())/(x.max()-x.min())
  return x

images = []
errors_file = open("errors.txt", "a")
error = 0
number = 0

for category in categories:
  img_per_category = 0
  idx = categories.index(category)
  for image in os.listdir(f"{path_to_dataset}/{category}"):
    if img_per_category == 1000:
      break
    path_to_image = f"{path_to_dataset}/{category}/{image}"
    try:
      img = cv.imread(path_to_image, cv.IMREAD_COLOR)
      img = cv.resize(img, (128,128))
      img = normalize(img)
      images.append([img, idx])
      img_per_category += 1

    except Exception as e:
      error += 1
      errors_file.write(f"{error}) {e}\n")
    finally:
      number += 1
    print(f"\rCalistirilan: {number} | Hatalar: {error}", end="")

errors_file.close()

with open("images_list.pickle", "wb") as f:
  pickle.dump(images, f)

images[0][0].shape

print(np.array(images)[:,1])

for _ in range(10):
  random.shuffle(images)

np.array(images)[:,1]

X = []
y = []
for image, idx in images:
  X.append(image)
  y.append(idx)

# x_train, y_train, x_val, y_val, x_tres, y_test verileri icin listeler olusturalim.
X_train = []
y_train = []

X_val = []
y_val = []

X_test = []
y_test = []

X_train = X[:1600]
y_train = y[:1600]

X_val = X[1600:1800]
y_val = y[1600:1800]

X_test = X[1800:]
y_test = y[1800:]

X_train = np.array(X_train)
y_train = np.array(y_train)

X_val = np.array(X_val)
y_val = np.array(y_val)

X_test = np.array(X_test)
y_test = np.array(y_test)

# dataseti ayirarak olusturdugumuz verileri yazdiralim.
print("x_train: ", len(X_train))
print("y_train: ", len(y_train))

print("x_val: ", len(X_val))
print("y_val: ", len(y_val))

print("x_test: ", len(X_test))
print("y_test: ", len(y_test))

# x_train, x_val ve x_test verilerinin boyutunu gorelim.
print(X_train[0].shape)
print(X_val[0].shape)
print(X_test[0].shape)

# x_train, y_train, x_val, y_val, x_test, y_test verilerini daha sonra kullanabilmek icin kaydedelim.
with open("X_train.pickle", "wb") as f:
  pickle.dump(X_train, f)  
with open("y_train.pickle", "wb") as f:
  pickle.dump(y_train, f)

with open("X_val.pickle", "wb") as f:
  pickle.dump(X_val, f)  
with open("y_val.pickle", "wb") as f:
  pickle.dump(y_val, f)

with open("X_test.pickle", "wb") as f:
  pickle.dump(X_test, f)  
with open("y_test.pickle", "wb") as f:
  pickle.dump(y_test, f)